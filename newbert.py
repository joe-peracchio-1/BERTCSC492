# -*- coding: utf-8 -*-
"""NewBert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-ekgbDVqJEUz5CJjD5AHo6y4v4ngS1w7
"""

#installing dataset
#using the prince by niccolo machiavelli (other books listed on report)
#link: https://www.gutenberg.org/ebooks/57037


#installing necessary packages
!pip install git+https://github.com/huggingface/transformers

!pip list | grep -E 'transformers|tokenizers'
# transformers version at notebook update --- 2.9.1
# tokenizers version at notebook update --- 0.7.0
!curl -L https://raw.githubusercontent.com/PacktPublishing/Transformers-for-Natural-Language-Processing/master/Chapter03/kant.txt --output "kant.txt"

# Commented out IPython magic to ensure Python compatibility.
# #@title Step 3: Training a Tokenizer
# 
# # print the CPU and wall times for the entire code
# %%time
# 
# from pathlib import Path
# from tokenizers import ByteLevelBPETokenizer
# 
# paths = 'Prince.txt'
# # Initialize a tokenizer
# tokenizer = ByteLevelBPETokenizer()
# 
# paths = [str(x) for x in Path(".").glob("**/*.txt")]
# 
# # Customize training
# tokenizer.train(files=paths, vocab_size=75_000, min_frequency=5,
# special_tokens=[
#     "<s>",
#     "<pad>",
#     "</s>",
#     "<unk>",
#     "<mask>",
# ])

import os
token_dir = '/content/MachiavelliBERT'
if not os.path.exists(token_dir):
  os.makedirs(token_dir)
tokenizer.save_model('MachiavelliBERT')

#@title Step 5 Loading the Trained Tokenizer Files

from tokenizers.implementations import ByteLevelBPETokenizer
from tokenizers.processors import BertProcessing
tokenizer = ByteLevelBPETokenizer(
    "./MachiavelliBERT/vocab.json",
    "./MachiavelliBERT/merges.txt",
)

#@title The tokenizer can now encode a sequence
tokenizer.encode("The Prince,").tokens

#@title The tokenizer can print the number of tokens in a sequence
tokenizer.encode("The Critique of Pure Reason.").tokens

!nvidia-smi
!pip install torchvision
!pip install torch==1.11.0+cu102 torchvision==0.12.0+cu102 -f https://download.pytorch.org/whl/torch_stable.html

import torch
torch.cuda.is_available()

!nvcc --version

from transformers import RobertaConfig
config = RobertaConfig(
    vocab_size=75_000,
    max_position_embeddings=512,
    num_attention_heads=24,
    num_hidden_layers=12,
    type_vocab_size=1,
)

from transformers import RobertaTokenizer

tokenizer._tokenizer.post_processor = BertProcessing(
    ("</s>", tokenizer.token_to_id("</s>")),  ## SEP token
    ("<s>", tokenizer.token_to_id("<s>")),    ## CLS token
)

tokenizer.enable_truncation(max_length=512)

tokenizer = RobertaTokenizer.from_pretrained("./MachiavelliBERT", max_length=512)

#tokenizer.encode("The Critique of Pure Reason.").tokens

from transformers import RobertaForMaskedLM

model = RobertaForMaskedLM(config=config)

print(model)

print(model.num_parameters())

LP=list(model.parameters())
lp=len(LP)
print(lp)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from transformers import LineByLineTextDataset
# 
# dataset = LineByLineTextDataset(
#     tokenizer=tokenizer,
#     file_path="./Prince.txt",
#     block_size=128,
# )

from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=True, mlm_probability=0.20
)

#import accelerate
#print(accelerate.__version__)

!pip uninstall accelerate -y
!pip install accelerate -U
!pip install transformers[torch]

from transformers import Trainer, TrainingArguments

!pip install transformers[torch]
!pip install accelerate -U

training_args = TrainingArguments(
    output_dir="./MachiavelliBERT",
    overwrite_output_dir=True,
    num_train_epochs=1,
    per_device_train_batch_size=32,
    save_steps=10_000,
    save_total_limit=2,
    )

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset,
)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# torch.cuda.empty_cache()
# trainer.train()

trainer.save_model("./MachiavelliBERT")

from transformers import pipeline

fill_mask = pipeline(
    "fill-mask",
    model="./MachiavelliBERT",
    tokenizer="./MachiavelliBERT"
)

#slowly
#fill_mask("Favours, on the other hand, should be given out <mask>, one by one, so that they can be properly savoured.")

(fill_mask("Favours,on the other hand, should be given out slowly, <mask> by one, so that they can be properly savoured."))

